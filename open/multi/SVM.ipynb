{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. combine `mon_features.pkl` & `unmon_features.pkl` into `features_df`"
      ],
      "metadata": {
        "id": "m5RXDI0jQISa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "MON_FILE_PATH = '/content/mon_features.pkl'\n",
        "UNMON_FILE_PATH = '/content/unmon_features.pkl'\n",
        "\n",
        "LABEL_COLUMN = ['website_label', 'monitored_label']\n",
        "\n",
        "FEATURES_VER3 = [\n",
        "    'total_transmission_time', 'std_inter_packet_time',\n",
        "    'avg_outgoing_burst_size', 'avg_incoming_burst_size',\n",
        "    'num_outgoing_packets', 'incoming_packet_ratio',\n",
        "    'outgoing_packet_ratio', 'cumul_packets_10pct',\n",
        "    'cumul_packets_30pct', 'outgoing_order_skew',\n",
        "    'incoming_order_skew', 'cumul_max', 'bigram_OO',\n",
        "    'num_incoming_first_30', 'outgoing_first_30',\n",
        "    'avg_incoming_order_first_30', 'avg_outgoing_order_first_30'\n",
        "]\n",
        "\n",
        "mon_features_df = pd.read_pickle(MON_FILE_PATH)\n",
        "unmon_features_df = pd.read_pickle(UNMON_FILE_PATH)\n",
        "\n",
        "features_df = pd.concat([mon_features_df, unmon_features_df], ignore_index=True)\n",
        "\n",
        "X = features_df[FEATURES_VER3]\n",
        "y = features_df[LABEL_COLUMN[0]]\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "405NfQbMQLcZ",
        "outputId": "72089dd0-740e-40c6-9ca8-6c0d904a228e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       total_transmission_time  std_inter_packet_time  \\\n",
            "0                        10.14               0.041168   \n",
            "1                        10.16               0.163930   \n",
            "2                        11.11               0.066661   \n",
            "3                        13.36               0.047809   \n",
            "4                        10.64               0.038760   \n",
            "...                        ...                    ...   \n",
            "28995                    32.09               0.163669   \n",
            "28996                    38.62               0.114350   \n",
            "28997                    34.93               1.331199   \n",
            "28998                    11.84               0.083521   \n",
            "28999                     9.62               0.026874   \n",
            "\n",
            "       avg_outgoing_burst_size  avg_incoming_burst_size  num_outgoing_packets  \\\n",
            "0                     1.551282                16.666667                 121.0   \n",
            "1                     1.702128                 9.319149                  80.0   \n",
            "2                     1.552632                16.315789                 118.0   \n",
            "3                     1.525000                16.550000                 122.0   \n",
            "4                     1.455696                16.341772                 115.0   \n",
            "...                        ...                      ...                   ...   \n",
            "28995                 1.619608                16.328125                 413.0   \n",
            "28996                 1.995536                20.724444                 447.0   \n",
            "28997                 2.107143                10.785714                  59.0   \n",
            "28998                 1.714286                 7.375000                  96.0   \n",
            "28999                 1.076923                32.226667                 322.0   \n",
            "\n",
            "       incoming_packet_ratio  outgoing_packet_ratio  cumul_packets_10pct  \\\n",
            "0                   0.914849               0.085151                 23.0   \n",
            "1                   0.845560               0.154440                  5.0   \n",
            "2                   0.913108               0.086892                  8.0   \n",
            "3                   0.915629               0.084371                 15.0   \n",
            "4                   0.918208               0.081792                 22.0   \n",
            "...                      ...                    ...                  ...   \n",
            "28995               0.910081               0.089919                 24.0   \n",
            "28996               0.912524               0.087476                  9.0   \n",
            "28997               0.836565               0.163435                240.0   \n",
            "28998               0.811395               0.188605                  7.0   \n",
            "28999               0.967768               0.032232                  4.0   \n",
            "\n",
            "       cumul_packets_30pct  outgoing_order_skew  incoming_order_skew  \\\n",
            "0                     55.0            -0.257072             0.021546   \n",
            "1                     49.0             0.153926            -0.037553   \n",
            "2                     34.0            -0.463423             0.038716   \n",
            "3                     57.0            -0.391122             0.031804   \n",
            "4                     53.0            -0.355596             0.027842   \n",
            "...                    ...                  ...                  ...   \n",
            "28995               1941.0            -0.272106             0.030423   \n",
            "28996               1656.0            -0.550236             0.044511   \n",
            "28997                357.0            -0.028954             0.010906   \n",
            "28998                 28.0            -0.703690             0.146871   \n",
            "28999                  8.0             0.045687             0.000038   \n",
            "\n",
            "       cumul_max  bigram_OO  num_incoming_first_30  outgoing_first_30  \\\n",
            "0            0.0   0.860563                   21.0           0.300000   \n",
            "1            0.0   0.756286                   22.0           0.266667   \n",
            "2            0.0   0.857775                   23.0           0.233333   \n",
            "3            0.0   0.860900                   21.0           0.300000   \n",
            "4            0.0   0.862633                   22.0           0.266667   \n",
            "...          ...        ...                    ...                ...   \n",
            "28995        0.0   0.854530                   20.0           0.333333   \n",
            "28996        0.0   0.868663                   19.0           0.366667   \n",
            "28997       -1.0   0.761111                   19.0           0.366667   \n",
            "28998       -1.0   0.702756                   21.0           0.300000   \n",
            "28999        0.0   0.937832                   21.0           0.300000   \n",
            "\n",
            "       avg_incoming_order_first_30  avg_outgoing_order_first_30  \n",
            "0                        13.619048                    16.555556  \n",
            "1                        14.318182                    15.000000  \n",
            "2                        14.913043                    13.142857  \n",
            "3                        13.619048                    16.555556  \n",
            "4                        14.318182                    15.000000  \n",
            "...                            ...                          ...  \n",
            "28995                    15.750000                    12.000000  \n",
            "28996                    14.631579                    14.272727  \n",
            "28997                    14.052632                    15.272727  \n",
            "28998                    14.666667                    14.111111  \n",
            "28999                    15.190476                    12.888889  \n",
            "\n",
            "[29000 rows x 17 columns]\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "28995   -1\n",
            "28996   -1\n",
            "28997   -1\n",
            "28998   -1\n",
            "28999   -1\n",
            "Name: website_label, Length: 29000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. SVM"
      ],
      "metadata": {
        "id": "vbjj7DOy-ajJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore ConvergenceWarning"
      ],
      "metadata": {
        "id": "T8DrseX8Xo-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"
      ],
      "metadata": {
        "id": "wei-xc2XXotK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into training and testing sets"
      ],
      "metadata": {
        "id": "ag7TNuQIXttl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "BzTWApNiXtNG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map all points to have mean=0 and std=1"
      ],
      "metadata": {
        "id": "5tGt6RZyX77d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scale = scaler.fit_transform(X_train.values)\n",
        "X_test_scale = scaler.transform(X_test.values)\n",
        "X_scale = scaler.fit_transform(X.values)"
      ],
      "metadata": {
        "id": "yVgWAxkuX8T7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test SVM using RBF kernel"
      ],
      "metadata": {
        "id": "QQ-XuWD_jrtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "clf_rbf = SVC(kernel='rbf', C=100, gamma=0.1, class_weight='balanced')\n",
        "clf_rbf.fit(X_train_scale, y_train)\n",
        "y_pred_rbf = clf_rbf.predict(X_test_scale)\n",
        "\n",
        "print(\"============ Before Hyperparameter tuning ============\")\n",
        "print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_rbf)))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rbf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9q3WURZjgco",
        "outputId": "c23230ad-e903-473b-f3a2-d98ce76278be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ Before Hyperparameter tuning ============\n",
            "SVM Accuracy: 0.8208275862068966\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.96      0.83      0.89      2470\n",
            "           0       0.74      0.76      0.75        42\n",
            "           1       0.89      0.80      0.85        51\n",
            "           2       0.57      0.96      0.71        47\n",
            "           3       0.67      0.94      0.78        51\n",
            "           4       0.64      0.79      0.71        43\n",
            "           5       0.86      0.86      0.86        44\n",
            "           6       0.83      0.92      0.88        53\n",
            "           7       0.80      0.65      0.72        55\n",
            "           8       0.79      0.72      0.76        47\n",
            "           9       0.63      0.71      0.67        45\n",
            "          10       0.63      0.75      0.69        44\n",
            "          11       0.66      0.82      0.73        55\n",
            "          12       0.74      0.94      0.83        52\n",
            "          13       0.57      0.54      0.56        57\n",
            "          14       0.69      0.74      0.72        58\n",
            "          15       0.76      0.89      0.82        46\n",
            "          16       0.77      0.69      0.73        59\n",
            "          17       0.64      0.76      0.70        51\n",
            "          18       0.89      0.96      0.92        50\n",
            "          19       0.58      0.62      0.60        45\n",
            "          20       0.93      0.98      0.95        52\n",
            "          21       0.66      0.81      0.73        48\n",
            "          22       0.74      0.74      0.74        47\n",
            "          23       0.73      0.88      0.80        42\n",
            "          24       0.64      0.75      0.69        48\n",
            "          25       0.80      0.74      0.77        54\n",
            "          26       0.81      0.91      0.86        47\n",
            "          27       0.87      0.91      0.89        57\n",
            "          28       0.87      0.85      0.86        48\n",
            "          29       0.67      0.87      0.76        39\n",
            "          30       0.75      0.88      0.81        43\n",
            "          31       0.79      0.84      0.81        49\n",
            "          32       0.81      0.70      0.75        54\n",
            "          33       0.63      0.80      0.70        54\n",
            "          34       0.58      0.51      0.54        49\n",
            "          35       0.67      0.85      0.75        40\n",
            "          36       0.83      0.91      0.87        44\n",
            "          37       0.78      0.72      0.75        60\n",
            "          38       0.85      0.75      0.80        55\n",
            "          39       0.74      0.80      0.77        46\n",
            "          40       0.73      0.74      0.74        58\n",
            "          41       0.92      0.84      0.88        57\n",
            "          42       0.59      0.68      0.63        47\n",
            "          43       0.73      0.92      0.81        49\n",
            "          44       0.90      1.00      0.95        46\n",
            "          45       0.64      0.52      0.57        52\n",
            "          46       0.65      0.87      0.74        45\n",
            "          47       0.74      0.78      0.76        54\n",
            "          48       0.49      0.89      0.64        47\n",
            "          49       0.93      0.83      0.88        52\n",
            "          50       0.84      0.88      0.86        49\n",
            "          51       0.83      0.82      0.83        61\n",
            "          52       0.91      0.92      0.92        53\n",
            "          53       0.70      0.93      0.80        40\n",
            "          54       0.84      0.81      0.82        57\n",
            "          55       0.69      0.80      0.74        54\n",
            "          56       0.96      0.98      0.97        49\n",
            "          57       0.84      0.92      0.88        39\n",
            "          58       0.85      0.85      0.85        52\n",
            "          59       0.89      0.96      0.92        51\n",
            "          60       0.84      0.80      0.82        54\n",
            "          61       0.85      0.70      0.76        56\n",
            "          62       0.75      0.80      0.78        61\n",
            "          63       0.76      0.82      0.79        55\n",
            "          64       0.69      0.88      0.77        42\n",
            "          65       0.82      0.78      0.80        59\n",
            "          66       0.72      0.71      0.72        41\n",
            "          67       0.86      0.88      0.87        58\n",
            "          68       0.77      0.70      0.73        47\n",
            "          69       0.58      0.87      0.70        45\n",
            "          70       0.90      0.98      0.94        47\n",
            "          71       0.86      0.93      0.89        45\n",
            "          72       0.80      0.76      0.78        58\n",
            "          73       0.85      0.91      0.88        58\n",
            "          74       0.83      0.73      0.77        66\n",
            "          75       0.89      1.00      0.94        55\n",
            "          76       0.88      0.98      0.92        44\n",
            "          77       0.70      0.64      0.67        59\n",
            "          78       0.67      0.83      0.74        53\n",
            "          79       0.61      0.57      0.59        53\n",
            "          80       0.89      0.92      0.91        53\n",
            "          81       0.82      0.69      0.75        45\n",
            "          82       0.78      0.78      0.78        55\n",
            "          83       0.82      0.92      0.87        50\n",
            "          84       0.93      0.79      0.85        48\n",
            "          85       0.90      0.98      0.94        54\n",
            "          86       0.92      0.92      0.92        51\n",
            "          87       0.60      0.80      0.69        51\n",
            "          88       0.86      0.88      0.87        49\n",
            "          89       0.63      0.61      0.62        44\n",
            "          90       0.90      0.82      0.86        44\n",
            "          91       0.84      0.86      0.85        37\n",
            "          92       0.70      0.73      0.72        52\n",
            "          93       0.92      0.93      0.92        58\n",
            "          94       0.76      0.68      0.72        50\n",
            "\n",
            "    accuracy                           0.82      7250\n",
            "   macro avg       0.77      0.82      0.79      7250\n",
            "weighted avg       0.84      0.82      0.82      7250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Hyperparameter tuning by using Grid Search"
      ],
      "metadata": {
        "id": "e-3oRgY2-og7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'gamma': [0.001, 0.01, 0.1, 1],\n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "\n",
        "search = HalvingGridSearchCV(SVC(class_weight='balanced'), param_grid, cv=5, factor=3, n_jobs=-1)\n",
        "search.fit(X_train_scale, y_train)\n",
        "\n",
        "print(search.best_params_)\n",
        "print(search.best_estimator_)"
      ],
      "metadata": {
        "id": "pE19k57F_PuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b9d8a8-cb9a-4271-f91f-a9c707bee28d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "SVC(C=100, class_weight='balanced', gamma=0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tuned = search.predict(X_test_scale)\n",
        "\n",
        "print(\"============ After Hyperparameter tuning ============\")\n",
        "print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_tuned)))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tuned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZmJZazXiuYm",
        "outputId": "4fa35965-e7f2-47e8-c631-6cdadef62221"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ After Hyperparameter tuning ============\n",
            "SVM Accuracy: 0.8208275862068966\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.96      0.83      0.89      2470\n",
            "           0       0.74      0.76      0.75        42\n",
            "           1       0.89      0.80      0.85        51\n",
            "           2       0.57      0.96      0.71        47\n",
            "           3       0.67      0.94      0.78        51\n",
            "           4       0.64      0.79      0.71        43\n",
            "           5       0.86      0.86      0.86        44\n",
            "           6       0.83      0.92      0.88        53\n",
            "           7       0.80      0.65      0.72        55\n",
            "           8       0.79      0.72      0.76        47\n",
            "           9       0.63      0.71      0.67        45\n",
            "          10       0.63      0.75      0.69        44\n",
            "          11       0.66      0.82      0.73        55\n",
            "          12       0.74      0.94      0.83        52\n",
            "          13       0.57      0.54      0.56        57\n",
            "          14       0.69      0.74      0.72        58\n",
            "          15       0.76      0.89      0.82        46\n",
            "          16       0.77      0.69      0.73        59\n",
            "          17       0.64      0.76      0.70        51\n",
            "          18       0.89      0.96      0.92        50\n",
            "          19       0.58      0.62      0.60        45\n",
            "          20       0.93      0.98      0.95        52\n",
            "          21       0.66      0.81      0.73        48\n",
            "          22       0.74      0.74      0.74        47\n",
            "          23       0.73      0.88      0.80        42\n",
            "          24       0.64      0.75      0.69        48\n",
            "          25       0.80      0.74      0.77        54\n",
            "          26       0.81      0.91      0.86        47\n",
            "          27       0.87      0.91      0.89        57\n",
            "          28       0.87      0.85      0.86        48\n",
            "          29       0.67      0.87      0.76        39\n",
            "          30       0.75      0.88      0.81        43\n",
            "          31       0.79      0.84      0.81        49\n",
            "          32       0.81      0.70      0.75        54\n",
            "          33       0.63      0.80      0.70        54\n",
            "          34       0.58      0.51      0.54        49\n",
            "          35       0.67      0.85      0.75        40\n",
            "          36       0.83      0.91      0.87        44\n",
            "          37       0.78      0.72      0.75        60\n",
            "          38       0.85      0.75      0.80        55\n",
            "          39       0.74      0.80      0.77        46\n",
            "          40       0.73      0.74      0.74        58\n",
            "          41       0.92      0.84      0.88        57\n",
            "          42       0.59      0.68      0.63        47\n",
            "          43       0.73      0.92      0.81        49\n",
            "          44       0.90      1.00      0.95        46\n",
            "          45       0.64      0.52      0.57        52\n",
            "          46       0.65      0.87      0.74        45\n",
            "          47       0.74      0.78      0.76        54\n",
            "          48       0.49      0.89      0.64        47\n",
            "          49       0.93      0.83      0.88        52\n",
            "          50       0.84      0.88      0.86        49\n",
            "          51       0.83      0.82      0.83        61\n",
            "          52       0.91      0.92      0.92        53\n",
            "          53       0.70      0.93      0.80        40\n",
            "          54       0.84      0.81      0.82        57\n",
            "          55       0.69      0.80      0.74        54\n",
            "          56       0.96      0.98      0.97        49\n",
            "          57       0.84      0.92      0.88        39\n",
            "          58       0.85      0.85      0.85        52\n",
            "          59       0.89      0.96      0.92        51\n",
            "          60       0.84      0.80      0.82        54\n",
            "          61       0.85      0.70      0.76        56\n",
            "          62       0.75      0.80      0.78        61\n",
            "          63       0.76      0.82      0.79        55\n",
            "          64       0.69      0.88      0.77        42\n",
            "          65       0.82      0.78      0.80        59\n",
            "          66       0.72      0.71      0.72        41\n",
            "          67       0.86      0.88      0.87        58\n",
            "          68       0.77      0.70      0.73        47\n",
            "          69       0.58      0.87      0.70        45\n",
            "          70       0.90      0.98      0.94        47\n",
            "          71       0.86      0.93      0.89        45\n",
            "          72       0.80      0.76      0.78        58\n",
            "          73       0.85      0.91      0.88        58\n",
            "          74       0.83      0.73      0.77        66\n",
            "          75       0.89      1.00      0.94        55\n",
            "          76       0.88      0.98      0.92        44\n",
            "          77       0.70      0.64      0.67        59\n",
            "          78       0.67      0.83      0.74        53\n",
            "          79       0.61      0.57      0.59        53\n",
            "          80       0.89      0.92      0.91        53\n",
            "          81       0.82      0.69      0.75        45\n",
            "          82       0.78      0.78      0.78        55\n",
            "          83       0.82      0.92      0.87        50\n",
            "          84       0.93      0.79      0.85        48\n",
            "          85       0.90      0.98      0.94        54\n",
            "          86       0.92      0.92      0.92        51\n",
            "          87       0.60      0.80      0.69        51\n",
            "          88       0.86      0.88      0.87        49\n",
            "          89       0.63      0.61      0.62        44\n",
            "          90       0.90      0.82      0.86        44\n",
            "          91       0.84      0.86      0.85        37\n",
            "          92       0.70      0.73      0.72        52\n",
            "          93       0.92      0.93      0.92        58\n",
            "          94       0.76      0.68      0.72        50\n",
            "\n",
            "    accuracy                           0.82      7250\n",
            "   macro avg       0.77      0.82      0.79      7250\n",
            "weighted avg       0.84      0.82      0.82      7250\n",
            "\n"
          ]
        }
      ]
    }
  ]
}